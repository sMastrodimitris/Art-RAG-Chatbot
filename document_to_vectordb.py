# -*- coding: utf-8 -*-
"""Document to VectorDB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PDckFubnSdhKtMbUqJd-Wn1Sr1Vjsnw9

# Transforming our Corpous using Embedings to a VectorDB

Firstly we import the saved JSON corpous file, then we split our corpous accordingly, after we use all-MiniLM-L6-v2 embeddings and finally create a vectordb. In the end we test how well it can respond to queries.

### Install and Import Libraries
"""

# Install required packages
!pip install langchain langchain_text_splitters langchain_community chromadb sentence-transformers

#Import Libraries
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
import os
import re
import json

"""### JSON Loader"""

# Load Data
def load_json_with_custom_parser(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # Fix unquoted property names
        pattern = r'(\n\s*)([a-zA-Z0-9_]+)(\s*:)'
        content_fixed = re.sub(pattern, r'\1"\2"\3', content)

        data = json.loads(content_fixed)
        print(f"Successfully loaded JSON with {len(data.keys())} top-level keys!")
        return data

    except Exception as e:
        print(f"Error loading JSON: {e}")
        return None

# Load the JSON file
file_path = "wikipedia_corpus.json"
data = load_json_with_custom_parser(file_path)

if not data:
    print("Could not load JSON.")
    exit()

"""### Langchain Splitter"""

# Prepare documents for splitting
documents = []
for key, value in data.items():
    # Clean up raw text
    raw_text = value.get('text', '').strip()
    if not raw_text:
        continue

    # Construct document
    document_text = raw_text
    documents.append({
        "text": document_text,
        "metadata": {
            "title": value.get('title', key),
            "source": value.get('url', 'Unknown')
        }
    })

print(f"Created {len(documents)} documents from JSON data")

# Split the documents using LangChain
# After testing the 500-50 size-overlap ratio was found to be optimal
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=100,
    separators=["\n\n", "\n", " ", ""]
)
split_documents = []

"""### Create the VectorDB (1-1.5h without GPU acceleration)"""

for doc in documents:
    chunks = text_splitter.split_text(doc["text"])
    for chunk in chunks:
        cleaned_chunk = chunk.strip()

        # Filter out uninformative or too-short chunks
        if len(cleaned_chunk) < 50:
            continue
        if cleaned_chunk.lower() in ["art", "content", "title", "source", "references"]:
            continue
        if cleaned_chunk.lower().startswith("title:") or cleaned_chunk.lower().startswith("source:"):
            continue

        split_documents.append({"content": cleaned_chunk, "metadata": doc["metadata"]})

print(f"Split into {len(split_documents)} high-quality document chunks")

# Initialize Hugging Face embeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
print("Initialized embedding model")

# Create a ChromaDB from documents
texts = [doc["content"] for doc in split_documents]
metadatas = [doc["metadata"] for doc in split_documents]

# Create an output directory for ChromaDB
persist_directory = "chroma_db"
os.makedirs(persist_directory, exist_ok=True)

# Optional: show a preview
if texts:
    print("\nSample document:")
    print(texts[0][:200] + "...")
    print(f"Metadata: {metadatas[0]}")

# Create and persist the vector database
print("\nCreating ChromaDB (this may take some time for large datasets)...")
vectordb = Chroma.from_texts(
    texts=texts,
    embedding=embeddings,
    metadatas=metadatas,
    persist_directory=persist_directory
)
vectordb.persist()

print(f"\nChromaDB created and persisted to {persist_directory}")

"""### Test VectorDB quality using query results"""

# Example queries
queries = [
    "What is stained glass?",
    "Legacy of Donatello",
    "man is the measure of all things"
]

# Perform similarity search for each query
print("\nExample Query Results:")
for query in queries:
    print(f"\nQuery: {query}")
    results = vectordb.similarity_search(query, k=10)
    for i, doc in enumerate(results):
        print(f"Result {i+1}:")
        #print(f"Content: {doc.page_content[:100]}...")
        print(f"Title: {doc.metadata.get('title', 'Unknown')}")
        print(f"Content: {doc.page_content}")
        print(f"Source: {doc.metadata.get('source', 'Unknown')}")
        print("-" * 50)

"""### Save CromaDB to a .zip File"""

import shutil

shutil.make_archive("chroma_db_backup", 'zip', persist_directory)

from google.colab import files

files.download("chroma_db_backup.zip")