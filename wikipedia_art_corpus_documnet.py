# -*- coding: utf-8 -*-
"""Wikipedia Art Corpus Documnet.ipynb

Automatically generated by Colab.

# Creating our Knowledge Corpus

Using the wikipediaapi we can both pull the contect of pages directly selected but also find new pages using the hyperlinks within the selected pages and even (if we want) go deeper. Then save our finding in a json file that we will use in next steps.

### Install and Import Libraries
"""

#Install Libraries
!pip install wikipedia-api wikipedia

#Import Libraries
import wikipediaapi
import time
import random
from collections import deque

"""### Wikipedia-api Scrapper and sub-functions"""

#Create our Scrapper
class WikipediaScraper:
    def __init__(self, language='en', user_agent='Wikipedia-API-Scraper'):
        self.wiki = wikipediaapi.Wikipedia(user_agent=user_agent, language=language)

        # Track already fetched pages to avoid duplicates
        self.fetched_pages = set()

        # Store articles with their content
        self.articles = {}

        # Delay between requests to not overload
        self.rate_limit_delay = 1

        # Enable debug printing
        #self.debug = True


    def get_article(self, title):

       #Filter already selected pages
        if title in self.fetched_pages:
            if self.debug:
                print(f"Article '{title}' already fetched, skipping.")
            return self.articles.get(title)

        #Save the title of the page, the context and the url
        page = self.wiki.page(title)
        if page.exists():
            self.fetched_pages.add(title)
            article_data = {
                'title': page.title,
                'text': page.text,
                'url': page.fullurl,
            }
            self.articles[title] = article_data

            #Print the info saved
            print(f"Fetched: {title}")
            time.sleep(self.rate_limit_delay)
            return article_data
        else:
            #Print where the page is not found
            print(f"Page '{title}' not found.")
            return None

    #Find new pages using urls, selection_method='diverse' to avoid selecting links alphabetically
    def get_links_from_page(self, title, max_links=None, selection_method='diverse'):

        page = self.wiki.page(title)
        if not page.exists():
            return []

        # Filter to only include main article links (namespace 0)
        main_links = []
        for link_title in page.links.keys():

            # Skip disambiguation pages, image files, and other non-content pages
            if ('.jpg' not in link_title and
                '.png' not in link_title and
                '.svg' not in link_title and
                '(disambiguation)' not in link_title and
                'ISSN' not in link_title):
                main_links.append(link_title)
            #main_links.append(link_title)


        # Randomise link selection
        if selection_method == 'alphabetical':
            selected_links = main_links[:max_links]
        elif selection_method == 'random':
            # Random selection
            if len(main_links) <= max_links:
                selected_links = main_links
            else:
                selected_links = random.sample(main_links, max_links)
        elif selection_method == 'diverse':

            # Take some from start, middle and end
            if len(main_links) <= max_links:
                selected_links = main_links
            else:
                # Calculate segments
                third = len(main_links) // 3
                links_per_segment = max_links // 3

                start_links = main_links[:third][:links_per_segment]
                middle_links = main_links[third:2*third][:links_per_segment]
                end_links = main_links[2*third:][:links_per_segment]

                # Combine and ensure we have max_links total (account for rounding)
                selected_links = start_links + middle_links + end_links
                if len(selected_links) < max_links:

                    # Add some random additional links to reach max_links
                    remaining = set(main_links) - set(selected_links)
                    if remaining:
                        additional = random.sample(list(remaining),
                                                 min(max_links - len(selected_links), len(remaining)))
                        selected_links.extend(additional)
        else:
            # Default to first N links
            selected_links = main_links[:max_links]

        #Report finding
        if self.debug:
            print(f"Found {len(selected_links)} main article links on page '{title}' using {selection_method} selection")
            if len(selected_links) > 0:
                print(f"Sample links: {selected_links[:3]}...")

        return selected_links

    # Wikipedia crawling using breadth-first search
    def breadth_first_crawl(self, seed_topics, max_depth=2, max_pages=50, max_links_per_page=25, selection_method='diverse'):#2
        queue = deque([(topic, 0) for topic in seed_topics])
        visited = set(seed_topics)

        #Print Debugging Information
        print(f"Starting breadth-first crawl with {len(seed_topics)} seed topics")
        print(f"Max depth: {max_depth}, Max pages: {max_pages}, Max links per page: {max_links_per_page}")
        print(f"Selection method: {selection_method}")

        while queue and len(self.fetched_pages) < max_pages:
            current_topic, current_depth = queue.popleft()

            # Skip already fetched
            if current_topic in self.fetched_pages:
                continue

            # Fetch current page
            article = self.get_article(current_topic)
            if not article:
                continue

            # If we haven't reached max depth, add links to the queue
            if current_depth < max_depth:
                links = self.get_links_from_page(current_topic, max_links_per_page, selection_method)

                for link in links:
                    if link not in visited:
                        visited.add(link)
                        queue.append((link, current_depth + 1))

        if len(self.fetched_pages) >= max_pages:
            print(f"Reached max pages limit ({max_pages})")
        elif not queue:
            print("Queue is empty, no more pages to fetch")

        print(f"Crawl complete. Fetched {len(self.fetched_pages)} unique articles.")
        return self.articles


    #Create Corpus
    def build_corpus_from_topics_and_links(self, seed_topics, depth, max_pages=100, max_links_per_page=50, selection_method='diverse'):

        #Print Debugging Information
        print(f"Building corpus from {len(seed_topics)} seed topics with depth {depth}")

        # Make sure max_pages is at least the number of seed topics
        if max_pages < len(seed_topics):
            print(f"Warning: max_pages ({max_pages}) is less than the number of seed topics ({len(seed_topics)})")
            print(f"Setting max_pages to {len(seed_topics) + (len(seed_topics) * max_links_per_page)}")
            max_pages = len(seed_topics) + (len(seed_topics) * max_links_per_page)

        #Run crawller
        articles = self.breadth_first_crawl(seed_topics, depth, max_pages, max_links_per_page, selection_method)

        return self.articles

"""### Pull the trigger"""

# Art-related Wikipedia Pages
art_topics = [
    "Art",
    "History of Art",
    "Aesthetics",
    "Beauty",
    "The arts",
    "Architecture",
    "Painting",
    "Sculpture",
    "Music",
    "Theatre",
    "Performing arts",
    "Religious art",
    "Art movement",
    "Culture",
    "Aesthetic interpretation",
    "List of most-visited art museums",
    "Periods in Western art history"
    "Art criticism",
]

# Create the scraper and build corpus
scraper = WikipediaScraper()

# Build corpus with appropriate settings
corpus = scraper.build_corpus_from_topics_and_links(
    seed_topics=art_topics,
    depth=1,
    max_links_per_page=100,
    max_pages=5000,
    selection_method='diverse'
)

# Print the number and name of collected pages
print(f"\nTotal unique articles: {len(corpus)}")
print(f"Articles collected:")
for i, (title, _) in enumerate(corpus.items(), 1):
    print(f"{i}. {title}")

"""### Print some examples"""

print(corpus["Art"]["title"])

print(corpus["Art"]["text"])

"""### Save to JSON File"""

import json

with open("wikipedia_corpus.json", "w", encoding="utf-8") as f:
    json.dump(corpus, f, ensure_ascii=False, indent=2)
