# -*- coding: utf-8 -*-
"""ART Claude RAG.ipynb

Automatically generated by Colab.

# Turning the CromaDB to a RAG using Claude

This is a bonus Script, as I finally decided to move forward with chapgpt3.5 turbo. So yes it is working as expected, but it lacks the gradio graphical element.

### Install and Import Libraries
"""

# Install required packages
!pip install langchain langchain_text_splitters langchain_community chromadb sentence-transformers langchain_anthropic anthropic

#Import Libraries
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.chat_models import ChatAnthropic
from langchain_anthropic import ChatAnthropic
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import os
import textwrap
import zipfile

"""### Load the VectorDB that we have created"""

# Unzip the Chroma DB
with zipfile.ZipFile("chroma_db_backup.zip", 'r') as zip_ref:
    zip_ref.extractall("chroma_db")

# Initialize embeddings
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
print("Initialized embedding model")

# Load Chroma vectorstore
vectordb = Chroma(
    persist_directory="chroma_db",
    embedding_function=embeddings
)
print("Loaded ChromaDB from disk")

"""### Set-Up our LLM"""

# Set your Anthropic API key
os.environ["ANTHROPIC_API_KEY"] = ""  # Replace with your actual API key

# Initialize Anthropic Claude
llm = ChatAnthropic(
    model="claude-3-opus-20240229",
    temperature=0,
    anthropic_api_key=os.environ["ANTHROPIC_API_KEY"],
    max_tokens=1000
)


# Create a detailed ai persona to give more character by providing a detailed prompt, different from the GPT-3.5 Implementation using trial and error
template = """
You are Marie-Claire Minucciano, a renowned French-Italian art critic with decades of experience and deep knowledge of art history. You are articulate, insightful but not very theatrical.

Context:
{context}

Question:
{question}

Instructions:
- Provide a comprehensive, detailed response using the information from the context.
- Your answers should be at least 150-250 words, rich with historical context, artistic analysis, and meaningful commentary.
- Feel free to make connections between different art movements, artists, and historical periods if supported by the context.
- Use vivid, descriptive language to help the listener visualize artworks and ideas.
- Make use of French and Italian expressions where helpful.
- If the answer is not found in the context, respond with: "Je regrette, but I don't have enough information about this subject."
- Do NOT begin your response with roleplay elements like "*clears throat*", "*smiles*", "*speaks in an accent*", or anything similar.

Answer:
"""

PROMPT = PromptTemplate(
    template=template,
    input_variables=["context", "question"]
)

"""### Set-Up our RAG chain"""

# Create retriever from vectorstore
retriever = vectordb.as_retriever(search_kwargs={"k": 10})


# Create the RAG chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
    chain_type_kwargs={"prompt": PROMPT}
)


#Print retreived documents
def format_docs(docs):
    formatted = []
    for i, doc in enumerate(docs):
        title = doc.metadata.get('title', 'Unknown')
        source = doc.metadata.get('source', 'Unknown')
        formatted.append(f"Document {i+1}:")
        formatted.append(f"Title: {title}")
        formatted.append(f"Source: {source}")
        formatted.append(f"Content: {textwrap.fill(doc.page_content, width=80)}")
        formatted.append("-" * 50)
    return "\n".join(formatted)


# Run the query through RAG
def query_rag_system(query):
    print(f"\nQuery: {query}")
    print("-" * 200)

    result = qa_chain({"query": query})
    answer = result["result"]
    source_docs = result["source_documents"]

    print("Answer:")
    print(textwrap.fill(answer, width=200))
    print("\nSource Documents:")
    print(format_docs(source_docs))
    print("=" * 200)

    return answer, source_docs

"""### Run some Examples"""

# Sample queries
queries = [
    "What is stained glass?",
    "Legacy of Donatello",
    "man is the measure of all things"
]

# Run the queries
for query in queries:
    query_rag_system(query)

"""### Free Run"""

# Interactive query system
def interactive_rag():
    print("\n" + "=" * 200)
    print("Interactive RAG System")
    print("Type 'exit' to quit")
    print("=" * 200)

    while True:
        query = input("\nEnter your question: ")
        if query.lower() == 'exit':
            break
        query_rag_system(query)

#interactive_rag()
